{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5badec6-f0be-4201-ba37-ec668af5a03e",
   "metadata": {},
   "source": [
    "<h1>Individual Reflection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17fc53-0082-45b4-8ada-6f96a1ae13a6",
   "metadata": {},
   "source": [
    "Listed in my personal data file there are multiple datasets and explorations of such datasets, through reading these resources I became aware of a loose structure followed by the example EDA resources. That being to visualise the data in some way and to use this visual and vague information to clean/ remove data where necassary. I also became aware of the the practice of replacing ordered categorical variables (eg. 'low risk', 'medium risk', 'high risk') into integers for analysis. \n",
    "\n",
    "However past the preprocessing stage, I began to feel much less comfortable with my understanding of the code provided, mainly creating neural networks and the effect of adding, changing or removing layers in a deep neural network. Furthermore a better unerstanding of dataframe manipulation would allow me to explore the data quicker and more efficiently. As stated above, a better familiarity of GGplots (R) and seaborn (Python) would help with creating good visualisations of data. Also a better understanding of scipy or SKlearn would be of use for me due to the large quantity of built in functions.\n",
    "\n",
    "Despite this I still feel relatively comfortable with the content at present, with previous courses involving collaborative coding helping greatly. I hope to gain more practice using the github online/desktop environment in the coming assessment as during this project we completed individual notebook files, which i feel has not let me grasp the technicalities of GitHub just yet.\n",
    "\n",
    "During the exploration of the Kaggle submitted code above, I decided to look into the Standard Scalar and Tramsform Documentation. Highlighted in this documentation was that the functiuon StandardScalar() is sensitive to outliers, which should be considered before use in the EDA stage. Standard scalar performs the simple calcuation of mean (u) and standard deviation (std) and replaces each variable with (x-u)/(std). As can be seen by the violin plot above, the scale of each attribute becomes much more similar to each other, allowing for better model prediction as larger scales can dominate models.\n",
    "\n",
    "\n",
    "PyTorch would have been another good library to use for the deep learning model however I am more comfortable using tensorflow and so used it instead.this reflects my own personal comfort zone and is something I could work on in future prjects. For the next project I would like to be more comfortable using seaborn for python and also GGplot for R as I have limited experience using both and through reading example code online found them both to be quite powerful tools.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
